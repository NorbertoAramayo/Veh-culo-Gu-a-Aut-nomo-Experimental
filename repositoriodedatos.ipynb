{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from ..log import get_logger\n",
    "logger = get_logger(__name__)\n",
    "class Tub(object):\n",
    "    \"\"\"\n",
    "    almacén de datos de sensores.\n",
    "    Acepta str, int, float, image_array, image, and array data types.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, inputs=None, types=None):\n",
    "        self.path = os.path.expanduser(path)\n",
    "        logger.info('path_in_tub: {}'.format(self.path))\n",
    "        self.meta_path = os.path.join(self.path, 'meta.json')\n",
    "        self.df = None\n",
    "\n",
    "        exists = os.path.exists(self.path)\n",
    "\n",
    "        if exists:\n",
    "            # load log and meta\n",
    "            logger.info('Tub exists: {}'.format(self.path))\n",
    "            with open(self.meta_path, 'r') as f:\n",
    "                self.meta = json.load(f)\n",
    "            self.current_ix = self.get_last_ix() + 1\n",
    "\n",
    "        elif not exists and inputs:\n",
    "            logger.info('Tub does NOT exist. Creating new tub...')\n",
    "            # create log and save meta\n",
    "            os.makedirs(self.path)\n",
    "            self.meta = {'inputs': inputs, 'types': types}\n",
    "            with open(self.meta_path, 'w') as f:\n",
    "                json.dump(self.meta, f)\n",
    "            self.current_ix = 0\n",
    "            logger.info('New tub created at: {}'.format(self.path))\n",
    "        else:\n",
    "            msg = \"La ruta del contenedor o carpeta que proporcionó no existe y no pasó ninguna metainformación (inputs & types)\" + \\\n",
    "                  \"para crear una nueva carpeta: Comprobar la ruta de la carpeta\"\n",
    "\n",
    "            raise AttributeError(msg)\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def get_last_ix(self):\n",
    "        index = self.get_index()\n",
    "        if len(index) >= 1:\n",
    "            return max(index)\n",
    "        return -1\n",
    "\n",
    "    def update_df(self):\n",
    "        df = pd.DataFrame([self.get_json_record(i) for i in self.get_index(shuffled=False)])\n",
    "        self.df = df\n",
    "\n",
    "    def get_df(self):\n",
    "        if self.df is None:\n",
    "            self.update_df()\n",
    "        return self.df\n",
    "\n",
    "    def get_index(self, shuffled=True):\n",
    "        files = next(os.walk(self.path))[2]\n",
    "        record_files = [f for f in files if f[:6] == 'record']\n",
    "\n",
    "        def get_file_ix(file_name):\n",
    "            try:\n",
    "                name = file_name.split('.')[0]\n",
    "                num = int(name.split('_')[1])\n",
    "            except:\n",
    "                num = 0\n",
    "            return num\n",
    "\n",
    "        nums = [get_file_ix(f) for f in record_files]\n",
    "\n",
    "        if shuffled:\n",
    "            random.shuffle(nums)\n",
    "        else:\n",
    "            nums = sorted(nums)\n",
    "\n",
    "        return nums\n",
    "\n",
    "\n",
    "   \n",
    "    def inputs(self):\n",
    "        return list(self.meta['inputs'])\n",
    "\n",
    "   \n",
    "    def types(self):\n",
    "        return list(self.meta['types'])\n",
    "\n",
    "    def get_input_type(self, key):\n",
    "        input_types = dict(zip(self.inputs, self.types))\n",
    "        return input_types.get(key)\n",
    "\n",
    "    def write_json_record(self, json_data):\n",
    "        path = self.get_json_record_path(self.current_ix)\n",
    "        try:\n",
    "            with open(path, 'w') as fp:\n",
    "                json.dump(json_data, fp)\n",
    "        except TypeError:\n",
    "            logger.warn('troubles with record: {}'.format(json_data))\n",
    "        except FileNotFoundError:\n",
    "            raise\n",
    "        except:\n",
    "            logger.error('Unexpected error: {}'.format(sys.exc_info()[0]))\n",
    "            raise\n",
    "\n",
    "    def get_num_records(self):\n",
    "        import glob\n",
    "        files = glob.glob(os.path.join(self.path, 'record_*.json'))\n",
    "        return len(files)\n",
    "\n",
    "    def make_record_paths_absolute(self, record_dict):\n",
    "        d = {}\n",
    "        for k, v in record_dict.items():\n",
    "            if type(v) == str: #filename\n",
    "                if '.' in v:\n",
    "                    v = os.path.join(self.path, v)\n",
    "            d[k] = v\n",
    "\n",
    "        return d\n",
    "\n",
    "    def check(self, fix=False):\n",
    "        \"\"\"\n",
    "        Repite todos los registros y se asegura de que se puedan cargar.\n",
    "        Opcionalmente, elimina los registros que causan problemas.\n",
    "        \"\"\"\n",
    "        logger.info('Checking tub: {}'.format(self.path))\n",
    "        logger.info('Found: {} records'.format(self.get_num_records()))\n",
    "        problems = False\n",
    "        for ix in self.get_index(shuffled=False):\n",
    "            try:\n",
    "                self.get_record(ix)\n",
    "            except:\n",
    "                problems = True\n",
    "                if fix == False:\n",
    "                    logger.warning('problems with record {} : {}'.format(ix, self.path))\n",
    "                else:\n",
    "                    logger.warning('problems with record {}, removing: {}'.format(ix, self.path))\n",
    "                    self.remove_record(ix)\n",
    "        if not problems:\n",
    "            logger.info('No problems found.')\n",
    "\n",
    "    def remove_record(self, ix):\n",
    "        \"\"\"\n",
    "       eliminar datos asociados con un registro\n",
    "        \"\"\"\n",
    "        record = self.get_json_record_path(ix)\n",
    "        os.unlink(record)\n",
    "\n",
    "    def put_record(self, data):\n",
    "        \"\"\"\n",
    "        Guarda valores como imágenes que no se pueden guardar en el registro csv \n",
    "        y devuelva un registro referente a esos valores guardados que se pueden guardar en un csv.\n",
    "        \"\"\"\n",
    "        json_data = {}\n",
    "\n",
    "        for key, val in data.items():\n",
    "            typ = self.get_input_type(key)\n",
    "\n",
    "            if typ in ['str', 'float', 'int', 'boolean']:\n",
    "                json_data[key] = val\n",
    "\n",
    "            elif typ is 'image':\n",
    "                name = self.make_file_name(key, ext='.jpg')\n",
    "                val.save(os.path.join(self.path, name))\n",
    "                json_data[key]=name\n",
    "\n",
    "            elif typ == 'image_array':\n",
    "                img = Image.fromarray(np.uint8(val))\n",
    "                name = self.make_file_name(key, ext='.jpg')\n",
    "                img.save(os.path.join(self.path, name))\n",
    "                json_data[key]=name\n",
    "\n",
    "            else:\n",
    "                msg = 'unknown type{}'.format(typ)\n",
    "                raise TypeError(msg)\n",
    "\n",
    "        self.write_json_record(json_data)\n",
    "        self.current_ix += 1\n",
    "        return self.current_ix\n",
    "\n",
    "    def get_json_record_path(self, ix):\n",
    "        #return os.path.join(self.path, 'record_'+str(ix).zfill(6)+'.json')  #fill zeros\n",
    "        return os.path.join(self.path, 'record_' + str(ix) + '.json')  #don't fill zeros\n",
    "\n",
    "    def get_json_record(self, ix):\n",
    "        path = self.get_json_record_path(ix)\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                json_data = json.load(fp)\n",
    "        except UnicodeDecodeError:\n",
    "            raise Exception('bad record: %d. You may want to run `python manage.py check --fix`' % ix)\n",
    "        except FileNotFoundError:\n",
    "            raise\n",
    "        except:\n",
    "            logger.error('Unexpected error: {}'.format(sys.exc_info()[0]))\n",
    "            raise\n",
    "\n",
    "        record_dict = self.make_record_paths_absolute(json_data)\n",
    "        return record_dict\n",
    "\n",
    "    def get_record(self, ix):\n",
    "        json_data = self.get_json_record(ix)\n",
    "        data = self.read_record(json_data)\n",
    "        return data\n",
    "\n",
    "    def read_record(self, record_dict):\n",
    "        data={}\n",
    "        for key, val in record_dict.items():\n",
    "            typ = self.get_input_type(key)\n",
    "\n",
    "            # cargar objetos que se guardaron como archivos separados\n",
    "            if typ == 'image_array':\n",
    "                img = Image.open((val))\n",
    "                val = np.array(img)\n",
    "\n",
    "            data[key] = val\n",
    "        return data\n",
    "\n",
    "    def make_file_name(self, key, ext='.png'):\n",
    "        #name = '_'.join([str(self.current_ix).zfill(6), key, ext])\n",
    "        name = '_'.join([str(self.current_ix), key, ext])  # don't fill zeros\n",
    "        name = name = name.replace('/', '-')\n",
    "        return name\n",
    "\n",
    "    def delete(self):\n",
    "        \"\"\" Elimina la carpeta y los archivos de esta carpeta. \"\"\"\n",
    "        import shutil\n",
    "        shutil.rmtree(self.path)\n",
    "\n",
    "    def shutdown(self):\n",
    "       \n",
    "        pass\n",
    "\n",
    "    def get_record_gen(self, record_transform=None, shuffle=True, df=None):\n",
    "        \"\"\"\n",
    "        Registros de devoluciones.\n",
    "\n",
    "        Parámetros\n",
    "        ----------\n",
    "        record_transform : function\n",
    "            \n",
    "        shuffle : bool\n",
    "            \n",
    "        df : numpy Dataframe\n",
    "            Si se especifica df, el generador utilizará los registros especificados en ese DataFrame. \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Listas de valores de tamaño batch_size.\n",
    "\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            df = self.get_df()\n",
    "\n",
    "        while True:\n",
    "            for _ in self.df.iterrows():\n",
    "                if shuffle:\n",
    "                    record_dict = df.sample(n=1).to_dict(orient='record')[0]\n",
    "\n",
    "                record_dict = self.read_record(record_dict)\n",
    "\n",
    "                if record_transform:\n",
    "                    record_dict = record_transform(record_dict)\n",
    "\n",
    "                yield record_dict\n",
    "\n",
    "    def get_batch_gen(self, keys=None, batch_size=128, record_transform=None, shuffle=True, df=None):\n",
    "        \"\"\"\n",
    "        Genera lotes de registros.\n",
    "\n",
    "        Además, cada registro de un lote tiene una lista de valores. Al especificar claves como un subconjunto de las entradas, se pueden filtrar los datos innecesarios.\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        keys : lista de strings\n",
    "            .\n",
    "        batch_size : int (El número de registros en un lote.)\n",
    "            \n",
    "\n",
    "        Devuelve\n",
    "        -------\n",
    "       Asignación de claves\n",
    "\n",
    "        \"\"\"\n",
    "        record_gen = self.get_record_gen(record_transform=record_transform, shuffle=shuffle, df=df)\n",
    "\n",
    "        if df is None:\n",
    "            df = self.get_df()\n",
    "\n",
    "        if keys is None:\n",
    "            keys = list(self.df.columns)\n",
    "\n",
    "        while True:\n",
    "            record_list = [ next(record_gen) for _ in range(batch_size) ]\n",
    "\n",
    "            batch_arrays = {}\n",
    "            for i, k in enumerate(keys):\n",
    "                arr = np.array([r[k] for r in record_list])\n",
    "                batch_arrays[k] = arr\n",
    "            yield batch_arrays\n",
    "\n",
    "    def get_train_gen(self, X_keys, Y_keys, batch_size=128, record_transform=None, df=None):\n",
    "        \"\"\"\n",
    "        Devuelve un conjunto de entrenamiento / validación.\n",
    "\n",
    "        The records are always shuffled.\n",
    "\n",
    "        Parámetros\n",
    "        ----------\n",
    "        X_keys : list of strings\n",
    "         \n",
    "        Y_keys : list of strings\n",
    "         \n",
    "\n",
    "        Devuelve\n",
    "        -------\n",
    "        Tupla (X, Y), donde X es una matriz bidimensional (len (X_keys) x batch_size) \n",
    "        e Y es una matriz bidimensional (len (Y_keys) x batch_size).\n",
    "\n",
    "        \"\"\"\n",
    "        batch_gen = self.get_batch_gen(X_keys + Y_keys,\n",
    "                                       batch_size=batch_size,\n",
    "                                       record_transform=record_transform,\n",
    "                                       df=df)\n",
    "\n",
    "        while True:\n",
    "            batch = next(batch_gen)\n",
    "            X = [batch[k] for k in X_keys]\n",
    "            Y = [batch[k] for k in Y_keys]\n",
    "            yield X, Y\n",
    "\n",
    "    def get_train_val_gen(self, X_keys, Y_keys, batch_size=128, train_frac=.8,\n",
    "                          train_record_transform=None, val_record_transform=None):\n",
    "        \"\"\"\n",
    "        Crear generadores para entrenamiento y validación.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_frac : float\n",
    "        \n",
    "        train_record_transform : function\n",
    "     \n",
    "        val_record_transform : function\n",
    "      \n",
    "\n",
    "        Devuelve\n",
    "        -------\n",
    "        Una tupla (train_gen, val_gen), donde train_gen es el generador del conjunto de entrenamiento y\n",
    "        val_gen el generador de conjuntos de validación....\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            self.update_df()\n",
    "\n",
    "        train_df = self.df.sample(frac=train_frac, random_state=200)\n",
    "        val_df = self.df.drop(train_df.index)\n",
    "\n",
    "        train_gen = self.get_train_gen(X_keys=X_keys, Y_keys=Y_keys, batch_size=batch_size,\n",
    "                                       record_transform=train_record_transform, df=train_df)\n",
    "\n",
    "        val_gen = self.get_train_gen(X_keys=X_keys, Y_keys=Y_keys, batch_size=batch_size,\n",
    "                                     record_transform=val_record_transform, df=val_df)\n",
    "\n",
    "        return train_gen, val_gen\n",
    "\n",
    "    def tar_records(self, file_path, start_ix=None, end_ix=None):\n",
    "        \"\"\"\n",
    "        Crea un archivo tar de los registros y metadatos de un contenedor.\n",
    "\n",
    "    \n",
    "        Parámetros\n",
    "        ----------\n",
    "        file_path : string\n",
    "            Ruta de destino del archivo tar creado\n",
    "        start_ix : int\n",
    "            Índice de comienzo. El valor predeterminado es 0.\n",
    "        end_ix : int\n",
    "            Índice final. Por defecto es el último índice.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Ruta al archivo tar\n",
    "        \"\"\"\n",
    "        if not start_ix:\n",
    "            start_ix = 0\n",
    "\n",
    "        if not end_ix:\n",
    "            end_ix = self.get_last_ix() + 1\n",
    "\n",
    "        with tarfile.open(name=file_path, mode='w:gz') as f:\n",
    "            for ix in range(start_ix, end_ix):\n",
    "                record_path = self.get_json_record_path(ix)\n",
    "                f.add(record_path)\n",
    "            f.add(self.meta_path)\n",
    "\n",
    "        return file_path\n",
    "\n",
    "\n",
    "class TubWriter(Tub):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TubWriter, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def run(self, *args):\n",
    "        \"\"\"\n",
    "        Acepta valores, los empareja con las claves de entrada y los guarda en el disco.\n",
    "        \"\"\"\n",
    "        assert len(self.inputs) == len(args)\n",
    "        record = dict(zip(self.inputs, args))\n",
    "        self.put_record(record)\n",
    "\n",
    "\n",
    "class TubReader(Tub):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TubReader, self).__init__(*args, **kwargs)\n",
    "        self.read_ix = 0\n",
    "\n",
    "    def run(self, *args):\n",
    "        \"\"\"\n",
    "        Toma las keys  de la carpeta recuperàndolas secuencialmente.\n",
    "        \"\"\"\n",
    "        if self.read_ix >= self.current_ix:\n",
    "            return None\n",
    "\n",
    "        record_dict = self.get_record(self.read_ix)\n",
    "        self.read_ix += 1\n",
    "        record = [record_dict[key] for key in args ]\n",
    "        return record\n",
    "\n",
    "\n",
    "class TubHandler():\n",
    "    def __init__(self, path):\n",
    "        self.path = os.path.expanduser(path)\n",
    "\n",
    "    def get_tub_list(self):\n",
    "        folders = next(os.walk(self.path))[1]\n",
    "        return folders\n",
    "\n",
    "    def next_tub_number(self):\n",
    "        def get_tub_num(tub_name):\n",
    "            try:\n",
    "                num = int(tub_name.split('_')[1])\n",
    "            except:\n",
    "                num = 0\n",
    "            return num\n",
    "\n",
    "        folders = self.get_tub_list()\n",
    "        numbers = [get_tub_num(x) for x in folders]\n",
    "        next_number = max(numbers+[0]) + 1\n",
    "        return next_number\n",
    "\n",
    "    def create_tub_path(self):\n",
    "        tub_num = self.next_tub_number()\n",
    "        date = datetime.datetime.now().strftime('%y-%m-%d')\n",
    "        name = '_'.join(['tub', str(tub_num).zfill(2), date])\n",
    "        tub_path = os.path.join(self.path, name)\n",
    "        return tub_path\n",
    "\n",
    "    def new_tub_writer(self, inputs, types):\n",
    "        tub_path = self.create_tub_path()\n",
    "        tw = TubWriter(path=tub_path, inputs=inputs, types=types)\n",
    "        return tw\n",
    "\n",
    "\n",
    "class TubImageStacker(Tub):\n",
    "    \n",
    "    def rgb2gray(self, rgb):\n",
    "        \"\"\"\n",
    "        Toma una imagen rgb numpy devuelva una nueva imagen convertida a escala de grises\n",
    "        \"\"\"\n",
    "        return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    def stack3Images(self, img_a, img_b, img_c):\n",
    "        \"\"\"\n",
    "        Convierte 3 imágenes rgb en escala de grises y colóquelas en una sola imagen de salida\n",
    "        \"\"\"\n",
    "        width, height, _ = img_a.shape\n",
    "\n",
    "        gray_a = self.rgb2gray(img_a)\n",
    "        gray_b = self.rgb2gray(img_b)\n",
    "        gray_c = self.rgb2gray(img_c)\n",
    "\n",
    "        img_arr = np.zeros([width, height, 3], dtype=np.dtype('B'))\n",
    "\n",
    "        img_arr[...,0] = np.reshape(gray_a, (width, height))\n",
    "        img_arr[...,1] = np.reshape(gray_b, (width, height))\n",
    "        img_arr[...,2] = np.reshape(gray_c, (width, height))\n",
    "\n",
    "        return img_arr\n",
    "\n",
    "    def get_record(self, ix):\n",
    "        \"\"\"\n",
    "        Obtiene el registro actual y dos anteriores y apilar las 3 imágenes en una sola imagen.\n",
    "        \"\"\"\n",
    "        data = super(TubImageStacker, self).get_record(ix)\n",
    "\n",
    "        if ix > 1:\n",
    "            data_ch1 = super(TubImageStacker, self).get_record(ix - 1)\n",
    "            data_ch0 = super(TubImageStacker, self).get_record(ix - 2)\n",
    "\n",
    "            json_data = self.get_json_record(ix)\n",
    "            for key, val in json_data.items():\n",
    "                typ = self.get_input_type(key)\n",
    "\n",
    "                #load objects that were saved as separate files\n",
    "                if typ == 'image':\n",
    "                    val = self.stack3Images(data_ch0[key], data_ch1[key], data[key])\n",
    "                    data[key] = val\n",
    "                elif typ == 'image_array':\n",
    "                    img = self.stack3Images(data_ch0[key], data_ch1[key], data[key])\n",
    "                    val = np.array(img)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "class TubTimeStacker(TubImageStacker):\n",
    "    \"\"\"\n",
    "    La idea aquí es obligar a la red a aprender hacia adelante en el tiempo.    \"\"\"\n",
    "\n",
    "    def __init__(self, frame_list, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        frame_list de [0, 10] apila los registros actuales y 10 cuadros juntos en un solo registro.\n",
    "\n",
    "        \"\"\"\n",
    "        super(TubTimeStacker, self).__init__(*args, **kwargs)\n",
    "        self.frame_list = frame_list\n",
    "\n",
    "    def get_record(self, ix):\n",
    "        \"\"\"\n",
    "        Apila los N registros en un solo.\n",
    "        Cada valor de clave tiene el índice de registro con un sufijo _N donde N es el desplazamiento del marco en los datos.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for i, iOffset in enumerate(self.frame_list):\n",
    "            iRec = ix + iOffset\n",
    "\n",
    "            try:\n",
    "                json_data = self.get_json_record(iRec)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for key, val in json_data.items():\n",
    "                typ = self.get_input_type(key)\n",
    "\n",
    "                # load only the first image saved as separate files\n",
    "                if typ == 'image' and i == 0:\n",
    "                    val = Image.open(os.path.join(self.path, val))\n",
    "                    data[key] = val\n",
    "                elif typ == 'image_array' and i == 0:\n",
    "                    d = super(TubTimeStacker, self).get_record(ix)\n",
    "                    data[key] = d[key]\n",
    "                else:\n",
    "                    \"\"\"\n",
    "                    agrega _offset a la clave usuario / ángulo :  usuario / ángulo_0\n",
    "                    \"\"\"\n",
    "                    new_key = key + \"_\" + str(iOffset)\n",
    "                    data[new_key] = val\n",
    "        return data\n",
    "\n",
    "\n",
    "class TubGroup(Tub):\n",
    "    def __init__(self, tub_paths_arg):\n",
    "        tub_paths = util.files.expand_path_arg(tub_paths_arg)\n",
    "        logger.info('TubGroup:tubpaths: {}'.format(tub_paths))\n",
    "        self.tubs = [Tub(path) for path in tub_paths]\n",
    "        self.input_types = {}\n",
    "\n",
    "        record_count = 0\n",
    "        for t in self.tubs:\n",
    "            t.update_df()\n",
    "            record_count += len(t.df)\n",
    "            self.input_types.update(dict(zip(t.inputs, t.types)))\n",
    "\n",
    "        logger.info('joining the tubs {} records together. This could take {} minutes.'.format(record_count,\n",
    "                                                                                         int(record_count / 300000)))\n",
    "\n",
    "        self.meta = {'inputs': list(self.input_types.keys()),\n",
    "                     'types': list(self.input_types.values())}\n",
    "\n",
    "        self.df = pd.concat([t.df for t in self.tubs], axis=0, join='inner')\n",
    "\n",
    "    \n",
    "    def inputs(self):\n",
    "        return list(self.meta['inputs'])\n",
    "\n",
    "  \n",
    "    def types(self):\n",
    "        return list(self.meta['types'])\n",
    "\n",
    "    def get_num_tubs(self):\n",
    "        return len(self.tubs)\n",
    "\n",
    "    def get_num_records(self):\n",
    "        return len(self.df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
